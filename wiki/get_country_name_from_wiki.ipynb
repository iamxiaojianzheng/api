{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.\t过滤关键词中其他字符，只保留汉字re.sub('[^\\u4e00-\\u9fa5]', '', 'a,.b,123.阿阿')\n",
    "2.\t对关键词本身进行分词、国家名称映射、国家名称统计，从大到小排序，取最大值对应的关键字。   \n",
    "3.\t通过wiki百科对英文关键词进行检索\n",
    "4.\t如果有道可用，则先用有道进行翻译，判断有道反馈的信息中是否包含这个关键词的国家信息，有则提出。否则通过百度百科对翻译出的中文关键字进行检索，提取出最有可能的国家名称。\n",
    "5.\t对2、3、4流程得出的结果进行优先级处理，2最高，3、4随机\n",
    "6.\t将上5步得出的结果做成键值对形式的词表，再进行关键字与词表进行映射匹配\n",
    "7.\t人工复查"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 分词： segment_list = jieba.cut_for_search(text)\n",
    "- 国家名称映射：segment_list = list(filter(lambda s: True if s in countries else False, segment_list))\n",
    "- 统计 counted_list = dict(Counter(list_all))\n",
    "- 排序（从大到小）sorted_list = sorted(counted_list.items(), key=operator.itemgetter(1), reverse=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ wiki_zh-cn_api\n",
    "```\n",
    "https://w.bk.wjbk.site/w/api.php?\n",
    "format=json&action=query&generator=search&gsrnamespace=0&gsrlimit=10&prop=pageimages|extracts&pilimit=max\n",
    "&exintro&explaintext&exsentences=1&exlimit=max&origin=*&gsrsearch={keyword}\n",
    "```\n",
    "\n",
    "+ baike_api\n",
    "```\n",
    "http://baike.baidu.com/api/openapi/BaikeLemmaCardApi?scope=103&format=json&appid=379020&bk_key={keyword}`\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import jieba\n",
    "import re\n",
    "import time\n",
    "import random\n",
    "import hashlib\n",
    "import gzip\n",
    "from urllib.parse import quote\n",
    "from lxml.etree import HTML\n",
    "from fake_useragent import UserAgent\n",
    "from collections import Counter\n",
    "from scrapy.selector import Selector\n",
    "import operator\n",
    "import json\n",
    "from json import JSONDecodeError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "countries_json_file = r'E:\\文档\\SpiderProject\\TestSpider\\全球国家.json'\n",
    "\n",
    "with open(countries_json_file, 'r', encoding='utf-8') as f:\n",
    "    countries = json.load(f)\n",
    "countries = set(countries)\n",
    "ua = UserAgent()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_chinese(text):\n",
    "    '''\n",
    "        判断是否为中文\n",
    "    '''\n",
    "    return True if re.findall('([\\u4e00-\\u9fa5]+)', text) else False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_wiki(keyword):\n",
    "    '''\n",
    "        通过关键词，获取wiki反馈的html信息\n",
    "    '''\n",
    "    # wiki_api = 'https://w.bk.wjbk.site/wiki/{}'\n",
    "    wiki_api = 'https://baike.baidu.com/item/{}'\n",
    "    headers = {\n",
    "        'User-Agent': ua.random,\n",
    "    }\n",
    "    url = wiki_api.format(quote(keyword), 'utf-8')\n",
    "    r = requests.get(url, headers=headers)\n",
    "    r.encoding = r.apparent_encoding\n",
    "    return r.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tags_text_by_xpath(text, xpath):\n",
    "    '''\n",
    "        通过xpath获取标签的文本信息\n",
    "    '''\n",
    "    # html = HTML(text)\n",
    "    # tags_text_list = html.xpath(xpath)\n",
    "    selector = Selector(text=text).xpath(xpath)\n",
    "    tags_text_list = selector.extract()\n",
    "    return tags_text_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_text_segment(text):\n",
    "    '''\n",
    "        结巴分词\n",
    "    '''\n",
    "    if isinstance(text, str):\n",
    "        return list(jieba.cut_for_search(text))\n",
    "    elif isinstance(text, list):\n",
    "        segment_list = list()\n",
    "        for w in text:\n",
    "            segment_list.extend(jieba.cut_for_search(w))\n",
    "        return segment_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_list_most_element(list_all):\n",
    "    '''\n",
    "        返回一个列表中元素个数最多的那个元素\n",
    "    '''\n",
    "    # 进行统计\n",
    "    counted_list = dict(Counter(list_all))\n",
    "    # 进行排序\n",
    "    sorted_list = sorted(counted_list.items(), key=operator.itemgetter(1), reverse=True)\n",
    "    return sorted_list[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_country_from_text(data):\n",
    "    '''\n",
    "        从字符串中提取出出现频率最高国家名称\n",
    "    '''\n",
    "    # 对字符串进行分词\n",
    "    segment_list = get_text_segment(data)\n",
    "    # 过滤非国家名称字符串\n",
    "    segment_list = list(filter(lambda s: True if s in countries else False, segment_list))\n",
    "    # 如果列表中含有国家名称则进行统计排序，返回频率最高的国家名称\n",
    "    if segment_list:\n",
    "        country = get_list_most_element(segment_list)\n",
    "        return country\n",
    "    return ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_html(url, encoding='utf-8'):\n",
    "    '''\n",
    "        获取网页源码\n",
    "    '''\n",
    "    headers = {\n",
    "        'User-Agent': ua.random,\n",
    "    }\n",
    "    r = requests.get(url, headers=headers)\n",
    "    r.encoding = r.apparent_encoding\n",
    "    return r.text\n",
    "#     return r.text.encode(encoding).decode('unicode_escape')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_text_by_xpath(html, xpath):\n",
    "    return Selector(text=html).xpath(xpath).extract()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_chinese_from_json(html):\n",
    "    try:\n",
    "        json_text = json.loads(html)\n",
    "    except JSONDecodeError as e:\n",
    "        return ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_country_from_wiki_on_keyword(keyword):\n",
    "    '''\n",
    "        通过关键词，获取wiki反馈的html信息\n",
    "    '''\n",
    "    wiki_api = \\\n",
    "    '''\n",
    "    https://w.bk.wjbk.site/w/api.php?\n",
    "format=json&action=query&generator=search&gsrnamespace=0&gsrlimit=10&prop=pageimages|extracts&pilimit=max\n",
    "&exintro&explaintext&exsentences=1&exlimit=max&origin=*&gsrsearch={}\n",
    "    '''\n",
    "    url = wiki_api.format(quote(keyword), 'utf-8')\n",
    "    html = get_html(url)\n",
    "    chinese_list = re.findall('[\\u4e00-\\u9fa5]+', html)\n",
    "    country = get_country_from_text(chinese_list)\n",
    "    if country:\n",
    "        return country\n",
    "    return ''\n",
    "#     else:\n",
    "#         return get_country_from_text(description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_country_from_baike_on_keyword(keyword):\n",
    "    '''\n",
    "        通过关键词，获取wiki反馈的html信息\n",
    "    '''\n",
    "    baike_api = 'https://baike.baidu.com/item/{}'\n",
    "    xpath = '//div[re:test(@class, \"lemma-summary|lemmaWgt-lemmaTitle|basic-info|poster\")]//text()'\n",
    "    \n",
    "    url = baike_api.format(quote(keyword), 'utf-8')\n",
    "    html = get_html(url)\n",
    "    text_list = get_text_by_xpath(html, xpath)\n",
    "    chinese_list = re.findall('[\\u4e00-\\u9fa5]+', ''.join(text_list))\n",
    "    country = get_country_from_text(chinese_list)\n",
    "    if country:\n",
    "        return country\n",
    "    return ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_url = 'https://w.bk.wjbk.site/w/api.php?format=json&action=query&generator=search&gsrnamespace=0&gsrlimit=10&prop=pageimages|extracts&pilimit=max&exintro&explaintext&exsentences=1&exlimit=max&origin=*&gsrsearch=首尔'\n",
    "html = get_html(test_url)\n",
    "get_country_from_wiki_on_keyword('Beijing')\n",
    "# '{\"errno\":2}' == get_country_from_baike_on_keyword('Beijing')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
